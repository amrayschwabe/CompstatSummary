\section{knn}
non param method: $\hat f(x) = \frac{1}{k} = \sum_N y_i$ where N is k nearest neighbors. If underlying model is linear, knn cannot outperform linear regression.
\textbf{Curse of dimensionality: } Performance of linear regression deteriorates moreee slowly than KNN
\textbf{K-fold crossvalidation for 1 NN classifier: } \textcolor{blue}{cv.knn1 <- function(x,y,K,preselect=FALSE,q=10){
  # quick checks of input:
  stopifnot(is.matrix(x), nrow(x)==length(y), 1<=K, K<=nrow(x),
            q>=1, q<=ncol(x))

  # randomly shuffle the rows:
  n <- length(y)
  ind.x <- sample(n, replace=FALSE)
  x <- x[ind.x,]
  y <- y[ind.x]
  
  # create K (roughly) equally sized folds:
  folds <- cut(seq(1,n),breaks=K,labels=FALSE)
  
  # perform K fold cross validation:
  error <- integer(K)
  for(i in 1:K){
    # Segment data by fold using the which() function 
    ind.test <- which(folds==i, arr.ind=TRUE)
    x.test <- x[ind.test,]
    y.test <- y[ind.test]
    x.train <- x[-ind.test,]
    y.train <- y[-ind.test]
    
    y.pred <- knn1(x.train, x.test, y.train)  
    error[i] <- sum(y.pred != y.test)
  }
  return(sum(error/n))
}}
