\section{KNN}
Non param. method: $\hat f(x) = \frac{1}{k} = \sum_N y_i$ where N is k nearest neighbors. If underlying model is linear, KNN cannot outperform linear regression.
\textbf{Curse of dimensionality: } Performance of linear regression deteriorates more slowly than KNN \\
\textbf{K-fold crossvalidation for 1 NN classifier: }
\begin{lstlisting}[language = R]
cv.knn1 <- function(x,y,K,preselect=FALSE,q=10){
  n <- length(y)
  ind.x <- sample(n, replace=FALSE)
  x <- x[ind.x,]
  y <- y[ind.x]
  folds <- cut(seq(1,n),breaks=K,labels=FALSE)
  error <- integer(K)
  for(i in 1:K)\{
    ind.test <- which(folds==i, arr.ind=TRUE)
    x.test <- x[ind.test,]
    y.test <- y[ind.test]
    x.train <- x[-ind.test,]
    y.train <- y[-ind.test]
    y.pred <- knn(x.train, x.test, y.train, k = 1)  
    error[i] <- sum(y.pred != y.test)
  }
  return(sum(error/n))
}
\end{lstlisting}